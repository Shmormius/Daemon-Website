name: Scrub Webpage and Update JSON

on:
  schedule:
    - cron: '0 0 * * *' # Runs daily at midnight
  push:
    branches:
      - main

jobs:
  update-json:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4

    - name: Run web scraper script
      run: python main-pages/blogs/dbd-stats/scrub_webpage.py

    - name: Check for changes
      id: check_changes
      run: |
        git add main-pages/blogs/dbd-stats/ShmormiusDBD.JSON
        git add main-pages/blogs/dbd-stats/StoneOceanDBD.JSON
        git add main-pages/blogs/dbd-stats/SzillaDBD.JSON
        if git diff --cached --quiet; then
          echo "No changes to commit."
          echo "has_changes=false" >> $GITHUB_ENV
        else
          echo "Changes detected."
          echo "has_changes=true" >> $GITHUB_ENV
        fi

    - name: Commit and push changes
      if: env.has_changes == 'true'
      run: |
        git config --global user.name 'Shmormius'
        git config --global user.email 'daemonkerrigan@gmail.com'
        git commit -m 'Update JSON file from web scraping'
        git push
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
